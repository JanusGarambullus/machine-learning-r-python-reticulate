---
title: "Nonlinear regression"
output: html_notebook
---

```{r}
library(tidyverse)
library(caret)
library(AppliedPredictiveModeling)
library(doParallel)
library(earth)
library(kernlab)
library(mlbench)
```

# Computing

## Neural networks

1. Remove highly correlated predictors
2. Preprocessing: center and scale

Yes, averaged models or neural nets in general are very susceptible to highly correlated variables. Therefore, it is necessary 
to remove highly correlated variables.

```{r}
data(solubility)
```

```{r}
# Finding correlations

too_high <- findCorrelation(cor(solTrainXtrans), 0.75)

length(too_high)
# 87 variables need to be removed
train <- solTrainXtrans[, -too_high]
test <- solTestXtrans[, -too_high]

train_sol <- solTrainY
test_sol <- solTestY
```

Alright, all the data is ready to feed into the model.

Now let's create some grid search parameters for the neural net.

```{r}
# This is essentially a cartesian product
nnet_grid <- expand.grid(.decay = c(0, 0.01, 0.1),
                         .size = c(1:10),
                         .bag = FALSE)


indx <- createFolds(train_sol, returnTrain = TRUE)
ctrl <- trainControl(method = "cv", index = indx)
```

```{r}
workers=makeCluster(16,type="SOCK")
registerDoParallel(workers)
```

```{r}
nnet_tune <- train(train, train_sol,
                   method = "avNNet",
                   tuneGrid = nnet_grid,
                   trControl = ctrl,
                   preProc = c("center", "scale"),
                   linout = T,
                   trace = T,
                   MaxNWts = 10 * (ncol(train) + 1) + 10 + 1,
                   maxit = 100)
```

Lesson learned: Neural networks take forever to finish

```{r}
nnet_tune
```

MAE is 0.57 with cross-validation. This is actually way better than my linear regression models.
But does the increased accuracy warrant the added complexity and computational time?

## Random Forest

```{r}
ranf_model <- train(train, train_sol,
                    method = "rf",
                    tuneLength = 10,
                    trControl = trainControl(method = "cv", number = 10))

ranf_model
```
Actually, my random forest does a better job than the neural network. This is pretty amazing.
I will have to try these methods on my Titanic dataset.

## MARS

Do I need to preprocess for MARS? The author doesn't.


```{r}
mars_grid <- expand.grid(.degree = 1:2, .nprune = 2:38)

mars_model <- train(solTrainXtrans, train_sol,
                    method = "earth",
                    tuneGrid = mars_grid,
                    trControl = trainControl(method = "cv", number = 10))

mars_model
```

This is even better than the random forest model. What is going on? MARS models are not actually crap?

## Variable importance

```{r}
# Neural net
varImp(nnet_tune)
```

```{r}
# Random forest
varImp(mars_model)
```
They pick completely different variables.

## SVM

Do I need to remove correlated predictors?

```{r}
svm_model <- train(solTrainXtrans, train_sol,
                   method = "svmRadial",
                   preproc = c("center", "scale"),
                   tuneLength = 14,
                   trControl = trainControl(method = "cv"))

svm_model
```
You have to be kidding. This model is the best yet. When is this going to stop?

What I learnt is that neural nets are difficult to run, random forests and SVM are easy, MARS is also easy.

```{r}
varImp(svm_model)
```

## KNN

For this model, I will remove highly correlated variables.

```{r}
knn_model <- train(train, train_sol,
                   method = "knn",
                   preProc = c("center", "scale"),
                   trControl = trainControl(method = "cv", number = 10),
                   tuneLength = 20)

knn_model
```
This model is kinda crap compared to the others that came before.

```{r}
varImp(knn_model)
```


# Exercises

7.1 Support vector machine

```{r}
x <- runif(100, min = 2, max = 10)
y <- sin(x) + rnorm(length(x)) * .25
sinData <- data.frame(x = x, y = y)

plot(x, y)

## Create a grid of x values to use for prediction
data_grid <- data.frame(x = seq(2, 10, length = 100))
```

a. Fit different 

```{r}
rbfSVM <- ksvm(x = x, y = y, data = sinData,
               kernel ="rbfdot", kpar = "automatic",
               C = 1000000, epsilon = 0.1)

modelPrediction <- predict(rbfSVM, newdata = data_grid)

## This is a matrix with one column. We can plot the
## model predictions by adding points to the previous plot
plot(x, y)
points(x = data_grid$x, y = modelPrediction[,1],
       type = "l", col = "blue")
```

There is a tiny possibility that maybe I overfit my model a little bit. Just a tiny bit. Nothing too serious.

```{r}
rbfSVM <- ksvm(x = x, y = y, data = sinData,
               kernel ="rbfdot", kpar = "automatic",
               C = 0.0000000001, epsilon = 0.1)

modelPrediction <- predict(rbfSVM, newdata = data_grid)

## This is a matrix with one column. We can plot the
## model predictions by adding points to the previous plot
plot(x, y)
points(x = data_grid$x, y = modelPrediction[,1],
       type = "l", col = "blue")
```
SVM with the lowest cost parameter is essentially a linear model.

```{r}
rbfSVM <- ksvm(x = x, y = y, data = sinData,
               kernel ="rbfdot", kpar = list(sigma = 6),
               C = 0.25, epsilon = 0.1)

modelPrediction <- predict(rbfSVM, newdata = data_grid)

## This is a matrix with one column. We can plot the
## model predictions by adding points to the previous plot
plot(x, y)
points(x = data_grid$x, y = modelPrediction[,1],
       type = "l", col = "blue")
```
This one looks quite good. I'm essentially doing manual grid search for the best parameters. The author wants me to get an intuitive understanding of what these do.
Epsilon flattens the line.

This works fine. I can't really tell the difference between different parameters. Both sigma and the cost go from 0 = underfitting / linear model to overfitting.
They are able to capture nonlinear relationships.

7.2 Friedman benchmark datasets

```{r}
training_data <- mlbench.friedman1(200, sd = 1)
training_data$x <- as.data.frame(training_data$x)

featurePlot(training_data$x, training_data$y)
```
Ok, what is this supposed to mean?

```{r}
test_data <- mlbench.friedman1(5000, sd = 1)
test_data$x <- data.frame(test_data$x)
```

Let's check for highly correlated variables. That's what we did in computing.

```{r}
corrplot::corrplot(cor(training_data$x), method = "number")
```
It looks great, there is no need to remove correlated predictors, I have
been handed clean data.

**KNN**

```{r}
## Tune example model on the data
## I only have 200 observations, so cross-validate heavily
knn_model <- train(training_data$x, training_data$y,
                   method = "knn",
                   preProc = c("center", "scale"),
                   tuneLength = 10,
                   trControl = trainControl(method = "repeatedcv", 
                                            number = 10, 
                                            repeats = 10))

knn_model
```

Is the MAE of 2.5 considered good? I would have to check my data for that.

```{r}
qplot(training_data$y)
```

```{r}
summary(training_data$y)
```
It's not horrible but also not very good.

```{r}
2.53 / (max(training_data$y) - min(training_data$y)) * 100
```
It tells us about 10% of the range. It could be improved.

**Random Forest**

```{r}
workers = makeCluster(detectCores(), type = "SOCK")
registerDoParallel(workers)

ranf_model <- train(training_data$x, training_data$y,
                    method = "rf",
                    tuneLength = 5,
                    verboseIter = T,
                    trainCtrl = trainControl(method = "repeatedcv",
                                             number = 10,
                                             repeats = 10))
```

That was pretty quick. I'm pushing this poor T420 with these full CPU load parallel model training processes.

```{r}
ranf_model
```
Much better than KNN. Yeah, KNN was crap before and crap now.

**SVM**

This one is a heavy-hitter. Let's see what it's capable of.

```{r}
svm_model <- train(training_data$x, training_data$y,
                   method = "svmRadial",
                   preProcess = c("center", "scale"),
                   tuneLength = 20,
                   trControl = trainControl(method = "repeatedcv",
                                            number = 10,
                                            repeats = 10))

svm_model
```
SVM training is so quick. Why is it so quick?

Yes, SVM is a real winner. I'm starting to like SVM a lot. They are really quick to train and they have pretty good accuracy. MAE of 1.71.

**OLS regression**

Let's do this.

```{r}
ols_model <- train(training_data$x, training_data$y,
                   method = "lm",
                   preProcess = c("center", "scale"),
                   trControl = trainControl(method = "repeatedcv",
                                            number = 10,
                                            repeats = 10))
ols_model
```
It's not very good but sort of on par with random forest.

**MARS**

This worked pretty well last time. Let's check it out.

```{r}
mars_model <- train(training_data$x, training_data$y,
                    method = "earth",
                    tuneLength = 30,
                    trControl = trainControl(method = "repeatedcv",
                                             number = 10,
                                             repeats = 10))
mars_model
```
Damn. This is better than the SVM. These MARS models are really no joke.
I never even heard of this model outside of statistical testbooks but these are good. Absolutely no joke.

It might be quite dataset dependent. Anyway, I will not disrespect MARS ever again.

**Neural Net**

i would rather not run it at home, my laptop won't take it. 

```{r}

```

I pretty much ran out of model for these chapters. Let's try one more thing.

**XGBoost**

```{r}
xgb_model <- train(training_data$x, training_data$y,
                   method = "xgbTree",
                   tuneLength = 5,
                   trControl = trainControl(method = "cv",
                                            number = 10))

xgb_model
```

```{r}
xgb_model$results %>% arrange(RMSE)
```
The best results has a MAE of 1.354 and RMSE of 1.72

xgboost is the winner so far. It has pretty amazing accuracy, but it's that much better than MARS, surprisingly even though noone talks about that model.


## Solubility with xgboost

```{r}
data(solubility)

workers = makeCluster(detectCores(), type = "SOCK")
registerDoParallel(workers)
```

```{r}
# Start the clock
ptm <- proc.time()

xgb_model <- train(solTrainXtrans, solTrainY,
                   method = "xgbTree",
                   verboseIter = T,
                   tuneLength = 4,
                   trControl = trainControl("repeatedcv", number = 10))

# Stop the clock
proc.time - ptm
```

Note that there are a lot of variables, 228 of them. Let's see if I can remove R = 1 for computational efficiency.
There is nothing with R = 1, so I'll leave all of them in.

```{r}
xgb_model$results %>% arrange(RMSE)
```

As you can see, the tunelength of 4 gave me 256 different models. This means there are 4 tuning parameters. There is an extra one, gamma, which was held constant.

```{r}
xgb_model
```
Yeah, there are 7 tuning parameters that can be set individually. That is a lot.
The limit here is definitely computational time. You can only do so much with a single computer.

Random forest for comparison:

```{r}
ranf_model <- train(solTrainXtrans, solTrainY,
                   method = "rf",
                   verboseIter = T,
                   tuneLength = 20,
                   trControl = trainControl("repeatedcv", number = 10))

ranf_model
```










