---
title: "Nonlinear regression"
output: html_notebook
---

```{r}
library(tidyverse)
library(caret)
library(AppliedPredictiveModeling)
library(doParallel)
library(earth)
library(kernlab)
library(mlbench)
```

# Computing

## Neural networks

1. Remove highly correlated predictors
2. Preprocessing: center and scale

Yes, averaged models or neural nets in general are very susceptible to highly correlated variables. Therefore, it is necessary 
to remove highly correlated variables.

```{r}
data(solubility)
```

```{r}
# Finding correlations

too_high <- findCorrelation(cor(solTrainXtrans), 0.75)

length(too_high)
# 87 variables need to be removed
train <- solTrainXtrans[, -too_high]
test <- solTestXtrans[, -too_high]

train_sol <- solTrainY
test_sol <- solTestY
```

Alright, all the data is ready to feed into the model.

Now let's create some grid search parameters for the neural net.

```{r}
# This is essentially a cartesian product
nnet_grid <- expand.grid(.decay = c(0, 0.01, 0.1),
                         .size = c(1:10),
                         .bag = FALSE)


indx <- createFolds(train_sol, returnTrain = TRUE)
ctrl <- trainControl(method = "cv", index = indx)
```

```{r}
workers=makeCluster(16,type="SOCK")
registerDoParallel(workers)
```

```{r}
nnet_tune <- train(train, train_sol,
                   method = "avNNet",
                   tuneGrid = nnet_grid,
                   trControl = ctrl,
                   preProc = c("center", "scale"),
                   linout = T,
                   trace = T,
                   MaxNWts = 10 * (ncol(train) + 1) + 10 + 1,
                   maxit = 100)
```

Lesson learned: Neural networks take forever to finish

```{r}
nnet_tune
```

MAE is 0.57 with cross-validation. This is actually way better than my linear regression models.
But does the increased accuracy warrant the added complexity and computational time?

## Random Forest

```{r}
ranf_model <- train(train, train_sol,
                    method = "rf",
                    tuneLength = 10,
                    trControl = trainControl(method = "cv", number = 10))

ranf_model
```
Actually, my random forest does a better job than the neural network. This is pretty amazing.
I will have to try these methods on my Titanic dataset.

## MARS

Do I need to preprocess for MARS? The author doesn't.


```{r}
mars_grid <- expand.grid(.degree = 1:2, .nprune = 2:38)

mars_model <- train(solTrainXtrans, train_sol,
                    method = "earth",
                    tuneGrid = mars_grid,
                    trControl = trainControl(method = "cv", number = 10))

mars_model
```

This is even better than the random forest model. What is going on? MARS models are not actually crap?

## Variable importance

```{r}
# Neural net
varImp(nnet_tune)
```

```{r}
# Random forest
varImp(mars_model)
```
They pick completely different variables.

## SVM

Do I need to remove correlated predictors?

```{r}
svm_model <- train(solTrainXtrans, train_sol,
                   method = "svmRadial",
                   preproc = c("center", "scale"),
                   tuneLength = 14,
                   trControl = trainControl(method = "cv"))

svm_model
```
You have to be kidding. This model is the best yet. When is this going to stop?

What I learnt is that neural nets are difficult to run, random forests and SVM are easy, MARS is also easy.

```{r}
varImp(svm_model)
```

## KNN

For this model, I will remove highly correlated variables.

```{r}
knn_model <- train(train, train_sol,
                   method = "knn",
                   preProc = c("center", "scale"),
                   trControl = trainControl(method = "cv", number = 10),
                   tuneLength = 20)

knn_model
```
This model is kinda crap compared to the others that came before.

```{r}
varImp(knn_model)
```


# Exercises

7.1 Support vector machine

```{r}
x <- runif(100, min = 2, max = 10)
y <- sin(x) + rnorm(length(x)) * .25
sinData <- data.frame(x = x, y = y)

plot(x, y)

## Create a grid of x values to use for prediction
data_grid <- data.frame(x = seq(2, 10, length = 100))
```

a. Fit different 

```{r}
rbfSVM <- ksvm(x = x, y = y, data = sinData,
               kernel ="rbfdot", kpar = "automatic",
               C = 1000000, epsilon = 0.1)

modelPrediction <- predict(rbfSVM, newdata = data_grid)

## This is a matrix with one column. We can plot the
## model predictions by adding points to the previous plot
plot(x, y)
points(x = data_grid$x, y = modelPrediction[,1],
       type = "l", col = "blue")
```

There is a tiny possibility that maybe I overfit my model a little bit. Just a tiny bit. Nothing too serious.

```{r}
rbfSVM <- ksvm(x = x, y = y, data = sinData,
               kernel ="rbfdot", kpar = "automatic",
               C = 0.0000000001, epsilon = 0.1)

modelPrediction <- predict(rbfSVM, newdata = data_grid)

## This is a matrix with one column. We can plot the
## model predictions by adding points to the previous plot
plot(x, y)
points(x = data_grid$x, y = modelPrediction[,1],
       type = "l", col = "blue")
```
SVM with the lowest cost parameter is essentially a linear model.

```{r}
rbfSVM <- ksvm(x = x, y = y, data = sinData,
               kernel ="rbfdot", kpar = list(sigma = 6),
               C = 0.25, epsilon = 0.1)

modelPrediction <- predict(rbfSVM, newdata = data_grid)

## This is a matrix with one column. We can plot the
## model predictions by adding points to the previous plot
plot(x, y)
points(x = data_grid$x, y = modelPrediction[,1],
       type = "l", col = "blue")
```
This one looks quite good. I'm essentially doing manual grid search for the best parameters. The author wants me to get an intuitive understanding of what these do.
Epsilon flattens the line.

This works fine. I can't really tell the difference between different parameters. Both sigma and the cost go from 0 = underfitting / linear model to overfitting.
They are able to capture nonlinear relationships.

7.2 Friedman benchmark datasets

```{r}
training_data <- mlbench.friedman1(200, sd = 1)
training_data$x <- as.data.frame(training_data$x)
```



