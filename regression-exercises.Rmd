---
title: "Regression Exercises"
output: html_notebook
---

```{r}
library(tidyverse)
library(caret)
library(AppliedPredictiveModeling)
library(corrplot)
library(MASS)
```

This document serves as my notes on this chapter as well as the worked exercises.

Note: R^2 value shows the percentage of the variation explained by the model. I belive this comes
with an extra layer of difficulty as the flexible models can overfit, thereby increasing the value.
It also doesn't provide information on predictive accuracy.

## Variance-Bias Trade-off

The mean squared error (MSE) has 3 components

* Irreducible noise
* Model variance
* Model bias

The simple average had a very low variance as it is quite stable but high bias.
The moving average on the other hand had low bias (followed the data reasonably well)
but small changes in the data threw it out of control - high variance.

Collinearity issues can increase model variance.

Underfitting: Low variance, High bias
Overfitting: High variance, Low bias

```{r}
observed <- c(0.22, 0.83, -0.12, 0.89, -0.23, -1.30, -0.15, -1.4,
 0.62, 0.99, -0.18, 0.32, 0.34, -0.30, 0.04, -0.87,
 0.55, -1.30, -1.15, 0.20)

predicted <- c(0.24, 0.78, -0.66, 0.53, 0.70, -0.75, -0.41, -0.43,
0.49, 0.79, -1.19, 0.06, 0.75, -0.07, 0.43, -0.42,
-0.25, -0.64, -1.26, -0.07)

results <- data.frame(observed, predicted)
```

```{r}
ggplot(results, aes(x = observed, y = predicted)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, col = "grey") +
  ggtitle("Observed vs Predicted Results")
```

```{r}
residuals_pred <- data.frame(residual_values, predicted)

ggplot(residuals_pred, aes(x = predicted, y = residual_values)) +
  geom_point() +
  #geom_smooth(method = "lm") +
  geom_hline(yintercept = 0, col = "grey") +
  ggtitle("Residuals vs Predicted Values")
```

This plot is essentially the last plot with its axis rotated towards predicted.
It looks relatively random but it seems to slightly underpredict at the higher end of the prediction range.

```{r}
residual_values <- observed - predicted
summary(residual_values)
```

```{r}
RMSE(predicted, observed)
R2(predicted, observed)
```

## Linear Models

Why would you use a linear model? It's because their coefficient estimates are highly interpretable and has real life implication.
They are good if your variables are not correlated and if there is a linear relationship between the independent and dependent variable.
Failing that, this method is not ideal. 

**Ordinary least squares regression aims to minimise bias!**

It also needs to have a lower number of predictors than observations. Collinearity needs to be mitigated.
PCA can be used for this purpose as well as removing the most correlated variables.

Furthermore, the variance inflation factor can diagnose multicollinearity.

Alternatively, dimensionality reduction regression techniques such as ridge, lasso and elastic net regression models can be used.

**Another issue is when the relationship between dependent and independent predictors are not linear**
In this case, individual relationships need to be observed. A diagnostic plot is the predicted vs residual plot. If there is a curve in the plot,
the relationship is probably not linear.

If the relationship is really complex, it's better to use more flexible models such as SVM and Neural nets.

Outliers can also mess up linear regression, as it tries to minimise sum of squared errors, things that are far are going to weigh a lot.
There is a method called Huber that can mitigate the effect of outliers.

There are no tuning parameters. But we need to be really careful using this model as there are many things that can go wrong.

The correlated variables can actually cause the model predictions to be worse.
In case of the highly correlated dataset, the conclusion was simply that a model that can tolerate collinearity would be much more useful.

PCA works well with linear regression because the principal components created will be uncorrelated. However, it really hurts the interpretability of
the model, which is the biggest advantage of using linear regression. Furthermore, PCA doesn't consider the reponse variable, only
the variability of the predictor space.

So overall, the author doesn't recommend using PCA with linear regression, instead he recommends using PLS when there are many variables or 
correlated variables.

PLS is like PCA but instead of finding the best combinations of variables in the predictor space, it finds the best
variables in respect to the dependent variable. 

PCR = Principal Component Regression.

In PLS as well as PCR, predictors should be centered and scaled.
Essentially, what they say is that PLS is superior to PCR.

The predictive power of PLS and PCR are very similar, PLS models are simpler than PCR.

These chapters really make me think that most of these introductory methods are simply not the best way to go, but 
they are mentioned first because it's simpler, more interpretable and just basic.

## Computing

### Loading the data
```{r}
data(solubility)

# This gives us the names of all objects in a  regex way, quite cool.
ls(pattern = "^solT")
```

### Ordinary Linear Regression

```{r}
training_data <- solTrainXtrans
training_data$solubility <- solTrainY
```

```{r}
corrplot(cor(training_data), method = "color", order = "hclust")
```

Most variables are not highly correlated, but there is definitely multicollinearity within the data.

```{r}
lm_fit_all_predictors <- lm(solubility ~ ., data = training_data)
summary(lm_fit_all_predictors)
```

```{r}
lm_pred_1 <- predict(lm_fit_all_predictors, solTestXtrans)
head(lm_pred_1)
```

```{r}
lm_values_1 <- data.frame(obs = solTestY, pred = lm_pred_1)
defaultSummary(lm_values_1)
```

This prediction captured 87% of variance in the dependent variable. It's not too bad.

Using the robust linear model. What is the robust linear model again? Huber approach?

```{r}
rlm_fit_all_predictors <- rlm(solubility ~ ., data = training_data)
```

```{r}
ctrl <- trainControl(method = "cv", number = 10)

lm_fit_1 <- train(x = solTrainXtrans, y = solTrainY,
                  method = "lm", trControl = ctrl)
```

```{r}
lm_fit_1
```
This is slightly better than the train/test split linear model.

For models built to explain, we need to look at residuals.

For models built to predict, residuals give us an idea about areas where the model doesn't perform well.

```{r}
plot(lm_fit_1$finalModel$residuals, lm_fit_1$pred)
```
They look random enough to me, I don't think there are any problems.

```{r}
xyplot(solTrainY ~ predict(lm_fit_1),
 type = c("p", "g"),
 xlab = "Predicted", ylab = "Observed")
```
The plot shows real vs predicted values. They are meant to fall along the diagonal line and randomly distributed along the line.


```{r}
xyplot(resid(lm_fit_1) ~ predict(lm_fit_1),
        type = c("p", "g"),
        xlab = "Predicted", ylab = "Residuals")
```

The residuals are meant to be randomly scattered along the horizontal axis. It's the same as the previous plot except flipped.

```{r}
cor_thres <- 0.9
too_high <- findCorrelation(cor(solTrainXtrans), cor_thres)

too_high
```

```{r}
corrpred <- names(solTrainXtrans[too_high])
corrpred
```
These are the columns that are recommended to be removed based on the correlation threshold.

```{r}
train_x_filtered <- solTrainXtrans[, -too_high]

lm_filtered <- train(train_x_filtered, solTrainY, method = "lm",
                     trControl = ctrl)

lm_filtered
```
This is a mistake in the book. The predictors were not filtered.
There doesn't seem to be any difference in predictive accuracy though.

**Robust Linear Regression**

RLM doesn't allow the covariance matrix to be singular. What is this guy talking about?

```{r}
# Build the RLM

rlm_pca <- train(train_x_filtered, solTrainY,
                 method = "rlm",
                 preProcess = "pca",
                 trControl = ctrl)

rlm_pca
```
MAE and RMSE are very similar. RLM gives much more accurate predictions, quite nice.

### Partial Least Squares

This will be the alternative of the PCR, aka principal coefficient regression. It functions similarly to PCA in practice, but
it is supervised. It is in theory better than PCR. Let's check it out.

```{r}
library(pls)

pls_fit <- plsr(solubility ~ ., data = training_data)

predict(pls_fit, solTestXtrans[1:5,], ncomp = 1:2)
```

```{r}
pls_tune <- train(solTrainXtrans, solTrainY,
                  method = "pls",
                  tuneLength = 20,
                  trControl = ctrl,
                  preProcess = c("center", "scale"))

plot(pls_tune)
pls_tune
```
PLS gave us better results than PCR. I shall remember that. But why would you use PLS if you could use random forest?

```{r}
ranf_model <- train(solTrainXtrans, solTrainY,
                  method = "rf",
                  tuneLength = 10,
                  trControl = ctrl)
```


### Penalised regression models

Ridge regression


The MASS package contains 

## Exercises

### 6.1 IR spectroscopy

```{r}
data(tecator)
```

```{r}
corrplot(cor(absorp), method = "color")
```
Jesus Christ, what is this? lol

Everything is positively correlated with everything.

1. Use PCA to determine the effective dimension of the data

```{r}
data <- as.data.frame(absorp)

trans <- preProcess(x = data,
                    method = c("BoxCox", "center", "scale", "pca"))

trans
```
Yep, as the correlation plot suggested, it only needs 2 variables to capture 95% of the variance within the data.
The dataset is very redundant.

```{r}
processed_data <- predict(trans, data)

corrplot(cor(processed_data), method = "color")
```

They are completely uncorrelated.

### Models

**OLS Regression**

```{r}
ols_model_1 <- train(data, endpoints[,1],
                   method = "lm",
                   trControl = trainControl(method = "cv", number = 10))

ols_model_1
```
97% of the variation was explained. I wouldn't trust it. I don't know if it's adjusted R^2 or not.

```{r}
ols_model_2 <- train(data, endpoints[,2],
                   method = "lm",
                   trControl = trainControl(method = "cv", number = 10))

ols_model_2
```

```{r}
ols_model_3 <-  train(data, endpoints[,3],
                   method = "lm",
                   trControl = trainControl(method = "cv", number = 10))

ols_model_3
```

Ok, next I will only use the first column. There are too many variables.

**OLS with centering, scaling and PCA**

```{r}
ols_model_processed <- train(data, endpoints[,1],
                             method = "lm",
                             trControl = trainControl(method = "cv", number = 10),
                             preProcess = c("BoxCox", "center", "scale", "pca"))

ols_model_processed
```
What is going on with this? The predictions are so much worse. So much for preprocessing.

**PLS**

```{r}
pls_model <- train(data, endpoints[,1],
                   method = "pls",
                   tuneLength = 40,
                   trControl = trainControl(method = "cv", number = 10),
                   preProcess = c("BoxCox", "center", "scale"))

pls_model
```

```{r}
plot(pls_model)
```

This is the best one yet, PLS with preprocessing.

**Ridge regression**

```{r}
ridgeGrid <- data.frame(.lambda = seq(0, 0.1, length = 15))

ridge_model <- train(data, endpoints[,1],
                     method = "ridge",
                     tuneGrid = ridgeGrid,
                     trControl = trainControl(method = "cv", number = 10),
                     preProcess = c("BoxCox", "center", "scale"))

ridge_model
```
It worked with a reasonable MAE value. Not as good as PLS but not as bad as ordinary regression and PCR.

Though, what does Lambda 0 mean? It means there is no penalty term for coefficients. Therefore, this is ordinary regression. Great.

**Lasso regression**

```{r}
lasso_grid <- data.frame(.fraction = seq(0, 0.1, length = 15))

lasso_model <- train(data, endpoints[,1],
                     method = "lasso",
                     tuneGrid = lasso_grid,
                     trControl = trainControl(method = "cv", number = 10),
                     preProcess = c("BoxCox", "center", "scale"))

lasso_model
```
Yes! I beat PLS with lasso.

**Elastic net**

This is the final model. It's time for the elastic net.

```{r}
enet_grid <- data.frame(.fraction = seq(0, 0.1, length = 15))

enet_model <- train(data, endpoints[,1],
                     method = "enet",
                     tuneLength = 30,
                     trControl = trainControl(method = "cv", number = 10),
                     preProcess = c("BoxCox", "center", "scale"))

enet_model
```
The two models are practically identical. Lambda set to 0 caused the model to behave as a lasso model,
no ridge parameter. The lasso fraction estimate is close to 0 again. That was all of the models.

**Model comparison**

```{r}
models <- resamples(list(OLS = ols_model_1, PCR = ols_model_processed, PLS = pls_model, ridge = ridge_model, lasso = lasso_model, elastic_net = enet_model))

summary(models)
```

```{r}
model_differences <- diff(models)
summary(model_differences)
```

These results suggest that it's actually quite accurate at predicting the performance.
There is no significant difference between the performance of lasso, ridge and elastic net models.
I think this is good enough.

The models are absolutely great at determining the content of water as a beginning.
I'm done with these exercises.

Thoughts: People use the Free lunch theorem as if there wasn't an inherent difference in models. It's a safe bet I think to use a random forest model instead of any
type of linear regression model.















